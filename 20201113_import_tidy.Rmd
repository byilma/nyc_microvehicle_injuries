---
title: "Pull and Tidy"
date: "11/27/2020"
output: 
  github_document:
  always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(httr)
library(purrr)
library(leaflet)
library(plotly)

options(scipen=999)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d

Sys.setenv('MAPBOX_TOKEN' = 'pk.eyJ1IjoiYWFicmFtb3Y5MCIsImEiOiJja2gyZm5obzQwNWIxMnFxc3phcWh1MWtwIn0.amAvJHtFkTl9XWJ68fh96Q')

```


```{r import MV crash, include=FALSE}
#Importing crash data using the NYC Open Data API. The API response is limited to 50,000 rows, so we need a function that can be used to page through these results to capture all crashes in 2020. 

# crash_api_2019a = function(offset) {
#   GET("https://data.cityofnewyork.us/resource/h9gi-nx95.csv", 
#       query = list("$where" = "crash_date between '2019-01-01T00:00:00' and '2019-5-31T12:00:00'", "$limit" = 50000, "$offset" = offset)) %>% 
#   content("parsed") %>%
#   as_tibble() 
# }
# crash_api_2019b = function(offset) {
#   GET("https://data.cityofnewyork.us/resource/h9gi-nx95.csv", 
#       query = list("$where" = "crash_date between '2019-06-01T00:00:00' and '2019-10-31T12:00:00'", "$limit" = 50000, "$offset" = offset)) %>% 
#   content("parsed") %>%
#   as_tibble() 
# }
# 
# crash_api_2020 = function(offset) {
#   GET("https://data.cityofnewyork.us/resource/h9gi-nx95.csv", 
#       query = list("$where" = "crash_date between '2020-01-01T00:00:00' and '2020-10-31T12:00:00'", "$limit" = 50000, "$offset" = offset)) %>% 
#   content("parsed") %>%
#   as_tibble() 
# }
# 
# offsets = c(0, 50000)
# crash_dat = bind_rows(map_df(offsets, crash_api_2019a), 
#                       map_df(offsets, crash_api_2019b),
#                       map_df(offsets, crash_api_2020))


# I was getting an error here when I tried to knit the document so I've modified this below:
crash_api = function(offset, limit = 50000) {
  GET("https://data.cityofnewyork.us/resource/h9gi-nx95.csv", 
      query = list("$where" = "crash_date between '2019-01-01T00:00:0' and '2020-10-31T12:00:00'", "$limit" = limit, "$offset" = offset)) %>% 
  content("parsed") %>%
  as_tibble() 
}
offsets = seq(25000, 275000, by = 50000)
crash_dat = bind_rows(crash_api(offset = 0, limit = 25000),
                    map_df(offsets, crash_api)) 
crash_dat = crash_dat %>% distinct
```


```{r, tidy MV crash}
#cleaning - transpose so vehicle types are listed in one column
crash_dat = 
  crash_dat %>% 
  mutate(
    dow = as.factor(weekdays(crash_date))
  ) %>%
  separate(crash_date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.integer(year),
         month = as.integer(month),
         day = as.integer(day)) %>%
  pivot_longer(
    vehicle_type_code1:vehicle_type_code_5,
    names_to = "vehicle_number",
    values_to = "vehicle_options",
    names_prefix = "vehicle_type_code"
   ) %>%
  drop_na(vehicle_options)
```

```{r filtering for microvehicles}
#Exploring the vehicle types so that we can limit to bicycles -- double check this list
bikes = crash_dat %>%
  filter(str_detect(vehicle_options, "[Bb]ike") | 
           str_detect(vehicle_options, "REVEL") | 
           str_detect(vehicle_options, "SCO")  |
           str_detect(vehicle_options, "MOP")   |
           str_detect(vehicle_options, "ELEC")  |
           str_detect(vehicle_options, "^E-")) %>% 
  filter(vehicle_options != "ESCOVATOR" & vehicle_options != "Bike" &
      str_detect(vehicle_options, "Dirt", negate = TRUE),
      str_detect(vehicle_options, "[Mm]otorbike", negate = TRUE)
           ) 
           
# Run to check what the filter ^ resulted in
# bikes %>%
#   group_by(vehicle_options) %>%
#   count() %>% View()
```

Microvehicle incidents in NYC _ a leaflet map -- I just commented this out to knit the document
```{r, include=FALSE}
# bikes %>%
#   drop_na(c(latitude, longitude)) %>% 
#    filter(latitude > 40) %>% #filter on this to get rid of lat/longitude with values of (0,0)
#   leaflet() %>% 
#   addProviderTiles(providers$CartoDB.Positron) %>% 
#   addCircleMarkers(~longitude, ~latitude, radius = 1)
```


```{r lm for injuries by month}
month_df=
  tibble(
    month = 1:12,
    month_name = factor(month.name, ordered = TRUE, levels = month.name)
  )

fit_injuries_month = crash_dat %>%
  group_by(year) %>%
  mutate(year_2020 = year - 2019) %>%
  nest(data = -month) %>%
  mutate(models = map(data, ~glm(number_of_persons_injured ~ year_2020:borough,
                                 family = "poisson", data = .x)),
         models = map(models, broom::tidy)) %>% 
  select(-data) %>% 
  unnest(models) %>%
  select(month, term, estimate, std.error, p.value) %>% 
  mutate(term = str_replace(term, "year_2020:borough", "2020 v. 2019, Borough: ")) %>%
  left_join(month_df, by = "month") %>%
  select(-month) %>%
  rename(month = month_name) %>%
  select(month, everything())

fit_injuries_month

```

It seems like what this is looking at is the number of persons injured per crash, and seeing if there is a significant difference between years (for each month.) Right? So we're hypothesizing that each crash would be more dangerous in 2020, (rather than that there are more crashes?) But sure enough, seems like 2020 is more dangerous overall, interesting! - Emma


```{r plot injuries by month}

fit_injuries_month %>% 
  filter(term != "(Intercept)" & month != "November" & month != "December") %>%
  mutate(term = str_replace(term, "2020 v. 2019, ", "")) %>%
  ggplot(aes(x = month, y = exp(estimate), color = term)) + 
  geom_point(show.legend = FALSE, aes(size = estimate, alpha = .7)) +
  geom_errorbar(aes(ymin = exp(estimate - (1.96*std.error)), 
                    ymax = exp(estimate + (1.96*std.error)))) +
  geom_hline(yintercept = 1, linetype="dashed", 
                color = "darkred", size = 1, alpha = .7) +
  labs(
    title = "Difference in Rate of Injuries Per Crash in 2020 v. 2019",
    x = "Month",
    y = "2020 v. 2019 Difference"
  ) +
  theme(legend.position="right", legend.title = element_blank(),
        text = element_text(size = 10),
        axis.text.x = element_text(angle = 90, hjust = 1, size = 8))

```




```{r line plot of crashes over time}
#for the line plot, we will want a summary of the number of bike crashes by day. The collisions variable calculates the number of unique crashes and the bikes variable calculates the total number of bikes that crashed (there can be mutliple bikes involved in one crash but this is rare)
crash_date_summ = 
  crash_dat %>%
  filter(str_detect(vehicle_options, "[Bb]ike")) %>%
  group_by(year, month, day) %>%
  summarize(
    collisions = n_distinct(collision_id),
    bikes = n()
  )


#simple plot just to see what the data look like
crash_date_summ %>%
  ggplot(aes(x = paste(month, day, sep="-"), y = collisions, 
             group = year, color = as.factor(year))) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE) + 
  labs(
    title = "Number of Collisions Over Time in January-October of 2019 and 2020",
    x = "Month and Day",
    y = "Number of Collisions"
    ) +
  theme(text = element_text(size = 15), 
        axis.text.x = element_text(angle = 60, hjust=1, size=10)) +
  scale_colour_discrete("Year")

```



```{r import bikes, include=FALSE}
#Importing bike data using the NYC Open Data API. The API response is limited to 50,000 rows, so we need a function that can be used to page through these results to capture all bike counts in 2019 and 2020. 

# 
# bike_api = function(offset, limit = 50000) {
#   GET("https://data.cityofnewyork.us/resource/uczf-rk3c.csv", 
#       query = list("$where" = "date between '2019-01-01T00:00:00' and '2020-10-31T12:00:00'", "$limit" = limit, "$offset" = offset)) %>% 
#   content("parsed") %>%
#   as_tibble() 
# }
# 
# 
# offsets = c(25000, 75000, 125000, 175000, 225000, 275000, 325000, 375000, 425000, 475000, 525000, 575000, 625000, 675000, 725000, 775000, 825000, 875000, 925000, 975000)
# bike_dat = bind_rows(bike_api(offset = 0, limit = 25000),
#                     map_df(offsets, bike_api))

## Note: We' were're having a really weird issue with importing the data. It seems that the import works 
## when the offset is 0, 50000, 150000, 250000, 350000, etc. BUT, won't work when it's 100000, 200000 or any other multiple of 100000. 
## It's very strange, and was causing us to have big missing chunks of data. It will even work with 100001, and with 99999, but not 100000.
## I think this partially fixed it, but now it's not importing dates past May. Maybe someone else has an idea?


#I think the code below works
bike_api = function(offset, limit = 50000) {
  GET("https://data.cityofnewyork.us/resource/uczf-rk3c.csv", 
      query = list("$where" = "date between '2019-01-01T00:00:00' and '2020-10-31T12:00:00'", "$limit" = limit, "$offset" = offset)) %>% 
  content("parsed") %>%
  as_tibble() 
}


offsets = seq(0, 900000, by = 50000)
bike_dat = map_df(offsets, bike_api)
```



```{r tidy bikes}
# Make df with aggregate bike counts by day. I noticed that there are some days with NAs - haven't explored yet but might want to look into if the bike counting operation itself was limited at all by covid.
bike_counts_aggregate = 
  bike_dat %>% 
  mutate(
    date_time = date,
    date = lubridate::date(date_time)
  ) %>% 
  group_by(date) %>% 
  summarize(total_daily_bikes = sum(counts, na.rm = TRUE))

# Look at simple graph of bike counts by day, analagous to the one of crashes above.
bike_counts_aggregate %>%
  ggplot(aes(x = date, y = total_daily_bikes)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE)

## Ok, even with the partial fix above, there seem to be bands of missing dates. I think there's something wrong with the import still.
```


Contributing Factor
```{r contributing_factor}
crash_factor = 
  bikes %>%
  filter(
    month %in% (3:10),
    number_of_cyclist_injured > 0
    ) %>%
  pivot_longer(
    contributing_factor_vehicle_1:contributing_factor_vehicle_5,
    names_to = "vehicle_factor_num",
    names_prefix = "contributing_factor_vehicle_",
    values_to = "contributing_factor"
   ) %>%
  mutate(
    covid = as.factor(if_else(year == 2019, 'Pre-COVID', 'COVID'))
  ) %>%
  group_by(covid, contributing_factor) %>%
  summarise(
    collisions = n_distinct(collision_id)
  ) %>%
  drop_na(contributing_factor) %>%
  ungroup(contributing_factor) %>%
  filter(contributing_factor != "Unspecified") %>%
  mutate(
    covid = fct_relevel(covid, 'Pre-COVID'),
    proportion = round(collisions / sum(collisions), 2) 
  ) 
  

crash_factor %>%
  group_by(covid) %>%
  top_n(n = 10, wt = proportion) %>%
  mutate(
    contributing_factor = fct_reorder(as.factor(contributing_factor), collisions, .desc = TRUE)
  ) %>%
  ggplot(aes(x = contributing_factor, y = proportion, fill = covid)) +
  geom_bar(stat = "identity") +
  facet_grid( . ~covid, scales = "free_x", space = "free_x") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), legend.position = "none") +
  labs(
    title = "Top Ten Contributing Factors for Crashes Involving Injuries to Cyclists",
    x = "",
    y = "Proportion of Crashes"
  )
```

# Map Visualizations

```{r, maps}
crash_map = 
  crash_dat %>% 
  drop_na(c(latitude, longitude)) %>% 
   filter(str_detect(vehicle_options, "[Bb]ike") | 
           str_detect(vehicle_options, "REVEL") | 
           str_detect(vehicle_options, "SCO")  |
           str_detect(vehicle_options, "MOP")   |
           str_detect(vehicle_options, "ELEC")  |
           str_detect(vehicle_options, "^E-")) %>% 
  filter(number_of_persons_injured > 0) %>% 
  filter(borough == "BRONX") %>% 
  mutate(
    text_label = str_c("Crash Time: ", crash_time, "\nNumber of Persons Injured: ", number_of_persons_injured)) %>% 
  plot_mapbox(x = ~longitude, y = ~latitude, color =~number_of_persons_injured, text =~text_label)%>% 
  layout(
    mapbox = list(
      zoom = 9,
      center = list(lat = 40.67, lon = -73.97)))

crash_map
```

